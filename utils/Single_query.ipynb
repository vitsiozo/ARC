{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary Libraries and load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from typing import List, Tuple\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI # To work with OpenAI\n",
    "from langchain_core.output_parsers import JsonOutputParser # To help with structured output\n",
    "from langchain_core.prompts import PromptTemplate # To help create our prompt\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # To help with defining what output structure we want\n",
    "\n",
    "# Get api key for chatgpt\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "#print (OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary called task_sets to hold the location of the test (challenges) and solution files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sets = {\n",
    "    'training' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_solutions.json',\n",
    "    },\n",
    "    'evaluation' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_solutions.json',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to load the challenges from the files and return the challenges and the corresponding solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks_from_file(task_set):\n",
    "    \n",
    "    with open(task_set['challenges'], \"r\") as tasks:\n",
    "        challenges = json.load(tasks)\n",
    "\n",
    "    with open(task_set['solutions'], \"r\") as tasks:\n",
    "        solutions = json.load(tasks)\n",
    "\n",
    "    return challenges, solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training tasks and solutions. Query for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'test': [   {   'input': [   [0, 0, 8, 0, 0],\n",
      "                                 [0, 8, 8, 0, 0],\n",
      "                                 [0, 0, 8, 0, 0],\n",
      "                                 [0, 0, 0, 0, 0],\n",
      "                                 [0, 0, 0, 0, 0]]}],\n",
      "    'train': [   {   'input': [   [8, 8, 0, 0, 0],\n",
      "                                  [8, 8, 0, 0, 0],\n",
      "                                  [0, 0, 0, 0, 0],\n",
      "                                  [0, 0, 0, 0, 0],\n",
      "                                  [0, 0, 0, 0, 0]],\n",
      "                     'output': [   [0, 0, 0, 0, 0],\n",
      "                                   [2, 2, 0, 0, 0],\n",
      "                                   [2, 2, 0, 0, 0],\n",
      "                                   [0, 0, 0, 0, 0],\n",
      "                                   [0, 0, 0, 0, 0]]},\n",
      "                 {   'input': [[0, 8, 0], [0, 0, 0], [0, 0, 0]],\n",
      "                     'output': [[0, 0, 0], [0, 2, 0], [0, 0, 0]]},\n",
      "                 {   'input': [   [0, 0, 0, 0, 0],\n",
      "                                  [0, 8, 8, 8, 0],\n",
      "                                  [0, 0, 0, 0, 0],\n",
      "                                  [0, 0, 0, 0, 0],\n",
      "                                  [0, 0, 0, 0, 0]],\n",
      "                     'output': [   [0, 0, 0, 0, 0],\n",
      "                                   [0, 0, 0, 0, 0],\n",
      "                                   [0, 2, 2, 2, 0],\n",
      "                                   [0, 0, 0, 0, 0],\n",
      "                                   [0, 0, 0, 0, 0]]}]}\n",
      "[   [   [0, 0, 0, 0, 0],\n",
      "        [0, 0, 2, 0, 0],\n",
      "        [0, 2, 2, 0, 0],\n",
      "        [0, 0, 2, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "#Set the id of the task we want to run\n",
    "task_id = 'a79310a0'\n",
    "\n",
    "pp.pprint(challenges[task_id])\n",
    "pp.pprint(solutions[task_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and set it up for access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o-mini', api_key=OPENAI_API_KEY, max_tokens=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the json object into a string that can be fed to the LLM\n",
    "\n",
    "challenge_tasks: dict a list of tasks\n",
    "task_id: str the id of the task we want to convert to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n",
    "    \n",
    "    json_task = challenge_tasks[task_id]\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    train_tasks = json_task['train']\n",
    "    test_task = json_task['test']\n",
    "\n",
    "    final_output = \"Training Examples\\n\"\n",
    "\n",
    "    for i, task in enumerate(train_tasks):\n",
    "        final_output += f\"Example {i + 1}: Input\\n[\"\n",
    "        for row in task['input']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "        final_output += f\"Example {i + 1}: Output\\n[\"\n",
    "\n",
    "        for row in task['output']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "\n",
    "    final_output += \"Test\\n[\"\n",
    "    for row in test_task[test_input_index]['input']:\n",
    "        final_output += f\"\\n{str(row)}\"\n",
    "\n",
    "    final_output += \"]\\n\\nYour Response:\"\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this with a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples\n",
      "Example 1: Input\n",
      "[\n",
      "[8, 8, 0, 0, 0],\n",
      "[8, 8, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],]\n",
      "\n",
      "Example 1: Output\n",
      "[\n",
      "[0, 0, 0, 0, 0],\n",
      "[2, 2, 0, 0, 0],\n",
      "[2, 2, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],]\n",
      "\n",
      "Example 2: Input\n",
      "[\n",
      "[0, 8, 0],\n",
      "[0, 0, 0],\n",
      "[0, 0, 0],]\n",
      "\n",
      "Example 2: Output\n",
      "[\n",
      "[0, 0, 0],\n",
      "[0, 2, 0],\n",
      "[0, 0, 0],]\n",
      "\n",
      "Example 3: Input\n",
      "[\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 8, 8, 8, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],]\n",
      "\n",
      "Example 3: Output\n",
      "[\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 2, 2, 2, 0],\n",
      "[0, 0, 0, 0, 0],\n",
      "[0, 0, 0, 0, 0],]\n",
      "\n",
      "Test\n",
      "[\n",
      "[0, 0, 8, 0, 0]\n",
      "[0, 8, 8, 0, 0]\n",
      "[0, 0, 8, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]]\n",
      "\n",
      "Your Response:\n"
     ]
    }
   ],
   "source": [
    "task_string = json_task_to_string(challenges, task_id, 0)\n",
    "print (task_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a prediction as a list of lists\n",
    "class ARCPrediction(BaseModel):\n",
    "    prediction: List[List] = Field(..., description=\"A prediction for a task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List]:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to get a prediction for\n",
    "    test_input_index: the index of your test input. 96% of tests only have 1 input.\n",
    "\n",
    "    Given a task, predict the test output\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the string representation of your task\n",
    "    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n",
    "    \n",
    "    # Set up a parser to inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=ARCPrediction)\n",
    "\n",
    "    # Create your prompt template. This is very rudimentary! You should edit this to do much better.\n",
    "    # For example, we don't tell the model what it's first attempt was (so it can do a different one), that might help!\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"You are a bot that is very good at solving puzzles. Below is a list of input and output pairs with a pattern.\" \n",
    "                    \"Identify the pattern, then apply that pattern to the test input to give a final output\"\n",
    "                    \"Just give valid json list of lists response back, nothing else. Do not explain your thoughts.\"\n",
    "                    \"{format_instructions}\\n{task_string}\\n\",\n",
    "        input_variables=[\"task_string\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Wrap up your chain with LCEL\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Optional, print out the prompt if you want to see it. If you use LangSmith you could view this there as well.\n",
    "    # print (f\"Prompt:\\n\\n{prompt.format(task_string=task_string)}\")\n",
    "    \n",
    "    # Finally, go get your prediction from your LLM. Ths will make the API call.\n",
    "    output = chain.invoke({\"task_string\": task_string})\n",
    "\n",
    "    # Because the output is structured, get the prediction key. If it isn't there, then just get the output\n",
    "    if isinstance(output, dict):\n",
    "        prediction = output.get('prediction', output)\n",
    "    else:\n",
    "        prediction = output\n",
    "\n",
    "    # Safety measure to error out if you don't get a list of lists of ints back. This will spark a retry later.\n",
    "    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n",
    "        print(\"Warning: Output must be a list of lists of integers.\")\n",
    "        print (f\"Errored Output: {prediction}\")\n",
    "        raise ValueError(\"Output must be a list of lists of integers.\")\n",
    "    \n",
    "    # Let's find the shape of our prediction\n",
    "    num_rows = len(prediction)\n",
    "    num_cols = len(prediction[0]) if num_rows > 0 else 0\n",
    "    print(f\"    Prediction Grid Size: {num_rows}x{num_cols}\\n\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(challenges, task_id, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3):\n",
    "    \"\"\"\n",
    "    challenges: dict a list of challenges. This should come directly from your _challenges file\n",
    "    task_id: str the specific task_id for which you want to get predictions\n",
    "    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n",
    "    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n",
    "\n",
    "    This function gets a prediction for a single task_id.\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to hold your submissions that you'll return after the prediction is made\n",
    "    submission = {}\n",
    "\n",
    "    task_attempts = []  # List to store all attempts for the current task\n",
    "\n",
    "    # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n",
    "    for t, pair in enumerate(challenges[task_id]['test']):\n",
    "        print(f\"Starting task {task_id}, pair #{t + 1}\")\n",
    "\n",
    "        # Dictionary to store attempts for the current test pair\n",
    "        pair_attempts = {}  \n",
    "\n",
    "        # Run through each prediction attempt\n",
    "        for attempt in range(1, NUM_ATTEMPTS + 1):\n",
    "            attempt_key = f\"attempt_{attempt}\"\n",
    "            pair_attempts[attempt_key] = []  # Init your attempt\n",
    "\n",
    "            # Try to get a prediction, with retries in case of failure\n",
    "            for retry in range(RETRY_ATTEMPTS):\n",
    "                try:\n",
    "                    print(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n",
    "                    prediction = get_task_prediction(challenge_tasks=challenges,\n",
    "                                                     task_id=task_id,\n",
    "                                                     test_input_index=t)\n",
    "\n",
    "                    # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n",
    "                    pair_attempts[attempt_key] = prediction\n",
    "                    break  # Break the retry loop if prediction is successful\n",
    "                except Exception as e:\n",
    "                    print(f\"Retrying: {e}\")\n",
    "                    if retry == RETRY_ATTEMPTS - 1:\n",
    "                        pair_attempts[attempt_key] = []  # Assign None if all retries fail\n",
    "\n",
    "        # After you get your attempts, append them to the task attempts\n",
    "        task_attempts.append(pair_attempts)\n",
    "\n",
    "    # Append the task attempts to the submission with the task_id as the key\n",
    "    submission[task_id] = task_attempts\n",
    "\n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test this for a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task a79310a0, pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 5x5\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 5x5\n",
      "\n",
      "{   'a79310a0': [   {   'attempt_1': [   [0, 0, 0, 0, 0],\n",
      "                                         [0, 0, 2, 0, 0],\n",
      "                                         [0, 0, 2, 0, 0],\n",
      "                                         [0, 0, 0, 0, 0],\n",
      "                                         [0, 0, 0, 0, 0]],\n",
      "                        'attempt_2': [   [0, 0, 0, 0, 0],\n",
      "                                         [0, 0, 2, 0, 0],\n",
      "                                         [0, 0, 2, 0, 0],\n",
      "                                         [0, 0, 0, 0, 0],\n",
      "                                         [0, 0, 0, 0, 0]]}]}\n",
      "Actual Solution: \n",
      "[   [   [0, 0, 0, 0, 0],\n",
      "        [0, 0, 2, 0, 0],\n",
      "        [0, 2, 2, 0, 0],\n",
      "        [0, 0, 2, 0, 0],\n",
      "        [0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "# Load up training tasks\n",
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "task_id = 'a79310a0'\n",
    "\n",
    "# Run the model on a single task\n",
    "submission = run_model(challenges, task_id)\n",
    "\n",
    "# Print the submission\n",
    "pp.pprint (submission)\n",
    "print(f\"Actual Solution: \")\n",
    "pp.pprint(solutions[task_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
