{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286df4a",
   "metadata": {},
   "source": [
    "Import all necessary packages\n",
    "Langchain is a python library that helps us integrate an LLM into the code\n",
    "\n",
    "Set api's as env variables and access them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dcb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain # Main LangChain import\n",
    "from langchain_openai import ChatOpenAI # To work with OpenAI\n",
    "from langchain_anthropic import ChatAnthropic # To work with Anthropic (optional)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # To work with Gemini (optional)\n",
    "from langchain_core.output_parsers import JsonOutputParser # To help with structured output\n",
    "from langchain_core.prompts import PromptTemplate # To help create our prompt\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # To help with defining what output structure we want\n",
    "\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get api key for chatgpt\n",
    "chatgpt_api = os.getenv('CHATGPT_API_KEY')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f40ab",
   "metadata": {},
   "source": [
    "Change to the directory where the tests are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce91abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files included\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_solutions.json\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_test_challenges.json\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_solutions.json\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/sample_submission.json\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_challenges.json\n",
      "/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_challenges.json\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024'\n",
    "\n",
    "print (\"Files included\")\n",
    "for dirname, _, filenames in os.walk(dataset_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624136a5",
   "metadata": {},
   "source": [
    "Define the evaluation and training challenges and solutions in a dictionary called task_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b30c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sets = {\n",
    "    'training' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_solutions.json',\n",
    "    },\n",
    "    'evaluation' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_solutions.json',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f2b78",
   "metadata": {},
   "source": [
    "Load the tasks from the files and return the challenges and solutions tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f18669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks_from_file(task_set):\n",
    "    \n",
    "    with open(task_set['challenges'], \"r\") as tasks:\n",
    "        challenges = json.load(tasks)\n",
    "\n",
    "    with open(task_set['solutions'], \"r\") as tasks:\n",
    "        solutions = json.load(tasks)\n",
    "\n",
    "    return challenges, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3646a90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': [{'input': [[1, 1, 2, 3, 3, 3, 8, 8, 4],\n",
       "    [1, 1, 2, 3, 3, 3, 8, 8, 4],\n",
       "    [1, 1, 2, 3, 3, 3, 8, 8, 4],\n",
       "    [1, 1, 2, 3, 3, 3, 8, 8, 4]]}],\n",
       " 'train': [{'input': [[1, 1, 1], [2, 2, 2], [1, 1, 1]],\n",
       "   'output': [[1], [2], [1]]},\n",
       "  {'input': [[3, 4, 6], [3, 4, 6], [3, 4, 6]], 'output': [[3, 4, 6]]},\n",
       "  {'input': [[2, 3, 3, 8, 1], [2, 3, 3, 8, 1], [2, 3, 3, 8, 1]],\n",
       "   'output': [[2, 3, 8, 1]]},\n",
       "  {'input': [[2, 2], [6, 6], [8, 8], [8, 8]], 'output': [[2], [6], [8]]},\n",
       "  {'input': [[4, 4, 4, 4],\n",
       "    [4, 4, 4, 4],\n",
       "    [2, 2, 2, 2],\n",
       "    [2, 2, 2, 2],\n",
       "    [8, 8, 8, 8],\n",
       "    [3, 3, 3, 3]],\n",
       "   'output': [[4], [2], [8], [3]]}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "challenges['746b3537']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b79938",
   "metadata": {},
   "source": [
    "Setting up the LLM for access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7be464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o', api_key=chatgpt_api, max_tokens=3000)\n",
    "\n",
    "## And incase you want to try Anthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', api_key=UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\"), max_tokens=3000)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"), max_tokens=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c577b3",
   "metadata": {},
   "source": [
    "Convert the json object into a string that can be fed to the LLM\n",
    "\n",
    "challenge_tasks: dict a list of tasks\n",
    "task_id: str the id of the task we want to convert to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5b3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n",
    "    \n",
    "    json_task = challenge_tasks[task_id]\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    train_tasks = json_task['train']\n",
    "    test_task = json_task['test']\n",
    "\n",
    "    final_output = \"Training Examples\\n\"\n",
    "\n",
    "    for i, task in enumerate(train_tasks):\n",
    "        final_output += f\"Example {i + 1}: Input\\n[\"\n",
    "        for row in task['input']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "        final_output += f\"Example {i + 1}: Output\\n[\"\n",
    "\n",
    "        for row in task['output']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "\n",
    "    final_output += \"Test\\n[\"\n",
    "    for row in test_task[test_input_index]['input']:\n",
    "        final_output += f\"\\n{str(row)}\"\n",
    "\n",
    "    final_output += \"]\\n\\nYour Response:\"\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf1613",
   "metadata": {},
   "source": [
    "Try this with a specific example task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a308bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Examples\n",
      "Example 1: Input\n",
      "[\n",
      "[1, 1, 1],\n",
      "[2, 2, 2],\n",
      "[1, 1, 1],]\n",
      "\n",
      "Example 1: Output\n",
      "[\n",
      "[1],\n",
      "[2],\n",
      "[1],]\n",
      "\n",
      "Example 2: Input\n",
      "[\n",
      "[3, 4, 6],\n",
      "[3, 4, 6],\n",
      "[3, 4, 6],]\n",
      "\n",
      "Example 2: Output\n",
      "[\n",
      "[3, 4, 6],]\n",
      "\n",
      "Example 3: Input\n",
      "[\n",
      "[2, 3, 3, 8, 1],\n",
      "[2, 3, 3, 8, 1],\n",
      "[2, 3, 3, 8, 1],]\n",
      "\n",
      "Example 3: Output\n",
      "[\n",
      "[2, 3, 8, 1],]\n",
      "\n",
      "Example 4: Input\n",
      "[\n",
      "[2, 2],\n",
      "[6, 6],\n",
      "[8, 8],\n",
      "[8, 8],]\n",
      "\n",
      "Example 4: Output\n",
      "[\n",
      "[2],\n",
      "[6],\n",
      "[8],]\n",
      "\n",
      "Example 5: Input\n",
      "[\n",
      "[4, 4, 4, 4],\n",
      "[4, 4, 4, 4],\n",
      "[2, 2, 2, 2],\n",
      "[2, 2, 2, 2],\n",
      "[8, 8, 8, 8],\n",
      "[3, 3, 3, 3],]\n",
      "\n",
      "Example 5: Output\n",
      "[\n",
      "[4],\n",
      "[2],\n",
      "[8],\n",
      "[3],]\n",
      "\n",
      "Test\n",
      "[\n",
      "[1, 1, 2, 3, 3, 3, 8, 8, 4]\n",
      "[1, 1, 2, 3, 3, 3, 8, 8, 4]\n",
      "[1, 1, 2, 3, 3, 3, 8, 8, 4]\n",
      "[1, 1, 2, 3, 3, 3, 8, 8, 4]]\n",
      "\n",
      "Your Response:\n"
     ]
    }
   ],
   "source": [
    "task_string = json_task_to_string(challenges, '746b3537', 0)\n",
    "print (task_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09debb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a prediction as a list of lists\n",
    "class ARCPrediction(BaseModel):\n",
    "    prediction: List[List] = Field(..., description=\"A prediction for a task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "483e24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List]:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to get a prediction for\n",
    "    test_input_index: the index of your test input. 96% of tests only have 1 input.\n",
    "\n",
    "    Given a task, predict the test output\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the string representation of your task\n",
    "    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n",
    "    \n",
    "    # Set up a parser to inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=ARCPrediction)\n",
    "\n",
    "    # Create your prompt template. This is very rudimentary! You should edit this to do much better.\n",
    "    # For example, we don't tell the model what it's first attempt was (so it can do a different one), that might help!\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"You are a bot that is very good at solving puzzles. Below is a list of input and output pairs with a pattern.\" \n",
    "                    \"Identify the pattern, then apply that pattern to the test input to give a final output\"\n",
    "                    \"Just give valid json list of lists response back, nothing else. Do not explain your thoughts.\"\n",
    "                    \"{format_instructions}\\n{task_string}\\n\",\n",
    "        input_variables=[\"task_string\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Wrap up your chain with LCEL\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Optional, print out the prompt if you want to see it. If you use LangSmith you could view this there as well.\n",
    "    # print (f\"Prompt:\\n\\n{prompt.format(task_string=task_string)}\")\n",
    "    \n",
    "    # Finally, go get your prediction from your LLM. Ths will make the API call.\n",
    "    output = chain.invoke({\"task_string\": task_string})\n",
    "\n",
    "    # Because the output is structured, get the prediction key. If it isn't there, then just get the output\n",
    "    if isinstance(output, dict):\n",
    "        prediction = output.get('prediction', output)\n",
    "    else:\n",
    "        prediction = output\n",
    "\n",
    "    # Safety measure to error out if you don't get a list of lists of ints back. This will spark a retry later.\n",
    "    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n",
    "        print(\"Warning: Output must be a list of lists of integers.\")\n",
    "        print (f\"Errored Output: {prediction}\")\n",
    "        raise ValueError(\"Output must be a list of lists of integers.\")\n",
    "    \n",
    "    # Let's find the shape of our prediction\n",
    "    num_rows = len(prediction)\n",
    "    num_cols = len(prediction[0]) if num_rows > 0 else 0\n",
    "    print(f\"    Prediction Grid Size: {num_rows}x{num_cols}\\n\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9701a7",
   "metadata": {},
   "source": [
    "Create a function that will run through the challenges and create a submission file compatible with ARC Prize 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9a5599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(challenges, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3, NUM_TASKS=None):\n",
    "    \"\"\"\n",
    "    challenges: dict a list of challenges. This should come directly from your _challenges file\n",
    "    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n",
    "    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n",
    "    NUM_TASKS: int, If set, this represents the the number of tasks you'd like to test. If None then the all challeneges will be tested\n",
    "\n",
    "    Loop through your challenges and produce a submission.json file you can submit for a score.\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to hold your submissions that you'll return after all predictions are made\n",
    "    submission = {}\n",
    "\n",
    "    # Run through each task in your challenge set\n",
    "    for i, task_id in enumerate(challenges):\n",
    "        task_attempts = []  # List to store all attempts for the current task\n",
    "\n",
    "        # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n",
    "        for t, pair in enumerate(challenges[task_id]['test']):\n",
    "            print(f\"Starting task #{i + 1} ({task_id}), pair #{t+1}\")\n",
    "\n",
    "            # Dictionary to store attempts for the current test pair\n",
    "            pair_attempts = {}  \n",
    "\n",
    "            # Run through each prediction attempt\n",
    "            for attempt in range(1, NUM_ATTEMPTS + 1):\n",
    "                attempt_key = f\"attempt_{attempt}\"\n",
    "                pair_attempts[attempt_key] = [] # Init your attempt\n",
    "\n",
    "                # Try to get a prediction, with retries in case of failure\n",
    "                for retry in range(RETRY_ATTEMPTS):\n",
    "                    try:\n",
    "                        print(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n",
    "                        prediction = get_task_prediction(challenge_tasks=challenges,\n",
    "                                                         task_id=task_id,\n",
    "                                                         test_input_index=t)\n",
    "                        \n",
    "                        # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n",
    "                        pair_attempts[attempt_key] = prediction\n",
    "                        break  # Break the retry loop if prediction is successful\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retrying: {e}\")\n",
    "                        if retry == RETRY_ATTEMPTS - 1:\n",
    "                            pair_attempts[attempt_key] = []  # Assign None if all retries fail\n",
    "\n",
    "            # After you get your attempts, append them to the task attempts\n",
    "            task_attempts.append(pair_attempts)\n",
    "\n",
    "        # Append the task attempts to the submission with the task_id as the key\n",
    "        submission[task_id] = task_attempts\n",
    "\n",
    "        # If you want to stop after N tasks, uncomment the below\n",
    "        if NUM_TASKS is not None and i + 1 == NUM_TASKS:\n",
    "            break\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec9838",
   "metadata": {},
   "source": [
    "Try this with an example from training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac2f2896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task #1 (007bbfb7), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "{'007bbfb7': [{'attempt_1': [[7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 7, 7, 7, 0, 7, 7, 0], [7, 0, 7, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 7, 7, 7, 0, 7, 7, 0]], 'attempt_2': [[7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 0, 7, 7, 0, 7, 7, 7, 0], [7, 0, 7, 7, 0, 7, 7, 7, 0], [7, 0, 7, 7, 0, 7, 7, 7, 0], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7], [7, 7, 0, 7, 0, 7, 7, 0, 7]]}]}\n"
     ]
    }
   ],
   "source": [
    "# Load up training tasks\n",
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "\n",
    "# Run the model on a single task\n",
    "submission = run_model(challenges, NUM_TASKS=1)\n",
    "\n",
    "# Print the submission\n",
    "print (submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9040d1",
   "metadata": {},
   "source": [
    "Create a function that will take the submission output and save it as a submission.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca61574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submission, file_name='submission.json'):\n",
    "    \"\"\"\n",
    "    Save a submission file to the specified file name\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(submission, file)\n",
    "\n",
    "    print (f\"Submission saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8ae27",
   "metadata": {},
   "source": [
    "Create a function to score the submission by comparing the submission.json file to the solutions file for the corresponding set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4ac7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission(submission_file_name, solutions) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    submission_file_name: str, the file name of your submission file\n",
    "    solutions: dict, the ground truth solutions you'd like to test against\n",
    "    \n",
    "    Read a submission from file, score it, then return the score\n",
    "    \"\"\"\n",
    "    print (f\"Scoring {submission_file_name}\\n\")\n",
    "\n",
    "    # Open your submission file\n",
    "    with open(submission_file_name, \"r\") as file:\n",
    "        submission = json.load(file)\n",
    "\n",
    "    total_score = 0\n",
    "    total_tasks = 0\n",
    "\n",
    "    # Loop through each task in your submission to grade it\n",
    "    for task_id, task_submission in submission.items():\n",
    "        total_tasks += 1\n",
    "        task_score = 0\n",
    "        num_pairs = len(task_submission)\n",
    "\n",
    "        # Go through each task. Most will only have 1\n",
    "        for pair_index, pair_attempts in enumerate(task_submission):\n",
    "            print(f\"Scoring Task {task_id} pair #{pair_index+1}\")\n",
    "            pair_correct = False\n",
    "\n",
    "            # Look at both of your attempts\n",
    "            for attempt_key, attempt in pair_attempts.items():\n",
    "                \n",
    "                # check to see if one is correct\n",
    "                if attempt == solutions[task_id][pair_index]:\n",
    "                    print(f\"Task Id {task_id} pair {pair_index+1} {attempt_key} matches solution\")\n",
    "                    pair_correct = True\n",
    "                    break # If it is correct, log it and break the loop\n",
    "\n",
    "            if pair_correct:\n",
    "                task_score += 1\n",
    "\n",
    "        task_score /= num_pairs\n",
    "        total_score += task_score\n",
    "\n",
    "    return {\n",
    "        'total_score': total_score,\n",
    "        'total_tasks_scored': total_tasks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567079de",
   "metadata": {},
   "source": [
    "This is a simple function that will load up the tasks, run the model, create a submission file and then score the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7501542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task_set='training', NUM_TASKS=None, submission_file_name='submission.json'):\n",
    "    # Load datasets\n",
    "    challenges, solutions = load_tasks_from_file(task_set=task_sets[task_set])\n",
    "\n",
    "    # # Run the model\n",
    "    submission = run_model(challenges, NUM_TASKS=NUM_TASKS)\n",
    "\n",
    "    # Create (and overwrite) a submission file\n",
    "    create_submission_file(submission, file_name=submission_file_name)\n",
    "\n",
    "    # Score the submission\n",
    "    score_result = score_submission(solutions = solutions, submission_file_name=submission_file_name)\n",
    "\n",
    "    print(f\"Final score: {score_result['total_score']} of {score_result['total_tasks_scored']} ({round(score_result['total_score']/score_result['total_tasks_scored'] * 100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfd3ed",
   "metadata": {},
   "source": [
    "### Run the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19e1240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task #1 (00576224), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "Retrying: Invalid json output: [\n",
      "[3, 2, 3, 2, 3, 2],\n",
      "[7, 8, 7, 8, 7, 8],\n",
      "[2, 3, 2, 3, 2, 3],\n",
      "[8, 7, 8, 7, 8, 7],\n",
      "[3, 2, 3, 2, 3, 2],\n",
      "[7, 8, 7, 8, 7, 8],\n",
      "]\n",
      "    Predicting attempt #1, retry #2\n",
      "    Prediction Grid Size: 6x6\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 6x6\n",
      "\n",
      "Starting task #2 (009d5c81), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 11x14\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 11x14\n",
      "\n",
      "Starting task #3 (00dbd492), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 10x20\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 8x20\n",
      "\n",
      "Starting task #4 (03560426), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 10x10\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 10x10\n",
      "\n",
      "Starting task #5 (05a7bcf2), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #1, retry #2\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #1, retry #3\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #1\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #2\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #3\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 19193 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Starting task #6 (0607ce86), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 8x22\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 7x22\n",
      "\n",
      "Starting task #7 (0692e18c), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 9x9\n",
      "\n",
      "Starting task #8 (070dd51e), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 7x20\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 8x20\n",
      "\n",
      "Starting task #9 (08573cc6), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "    Prediction Grid Size: 1x13\n",
      "\n",
      "    Predicting attempt #2, retry #1\n",
      "    Prediction Grid Size: 1x13\n",
      "\n",
      "Starting task #10 (0934a4d8), pair #1\n",
      "    Predicting attempt #1, retry #1\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #1, retry #2\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #1, retry #3\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #1\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #2\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "    Predicting attempt #2, retry #3\n",
      "Retrying: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, you requested 17088 tokens (14088 in the messages, 3000 in the completion). Please reduce the length of the messages or completion.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "Submission saved to submission.json\n",
      "Scoring submission.json\n",
      "\n",
      "Scoring Task 00576224 pair #1\n",
      "Task Id 00576224 pair 1 attempt_1 matches solution\n",
      "Scoring Task 009d5c81 pair #1\n",
      "Scoring Task 00dbd492 pair #1\n",
      "Scoring Task 03560426 pair #1\n",
      "Scoring Task 05a7bcf2 pair #1\n",
      "Scoring Task 0607ce86 pair #1\n",
      "Scoring Task 0692e18c pair #1\n",
      "Scoring Task 070dd51e pair #1\n",
      "Scoring Task 08573cc6 pair #1\n",
      "Scoring Task 0934a4d8 pair #1\n",
      "Final score: 1.0 of 10 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "main(task_set='evaluation', NUM_TASKS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdea84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
