{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6286df4a",
   "metadata": {},
   "source": [
    "Import all necessary packages\n",
    "Langchain is a python library that helps us integrate an LLM into the code\n",
    "\n",
    "Set api's as env variables and access them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9a9479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COMMAND_MODE': 'unix2003',\n",
       " 'CONDA_DEFAULT_ENV': 'base',\n",
       " 'CONDA_EXE': '/Users/vitsiozo/anaconda3/bin/conda',\n",
       " 'CONDA_PREFIX': '/Users/vitsiozo/anaconda3',\n",
       " 'CONDA_PROMPT_MODIFIER': '(base) ',\n",
       " 'CONDA_PYTHON_EXE': '/Users/vitsiozo/anaconda3/bin/python',\n",
       " 'CONDA_SHLVL': '1',\n",
       " 'HOME': '/Users/vitsiozo',\n",
       " 'LOGNAME': 'vitsiozo',\n",
       " 'MallocNanoZone': '0',\n",
       " 'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       " 'PATH': '/Users/vitsiozo/anaconda3/bin:/Users/vitsiozo/anaconda3/condabin:/Library/Frameworks/Python.framework/Versions/3.9/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/TeX/texbin:/Applications/Little Snitch.app/Contents/Components:/Users/vitsiozo/Library/Application Support/JetBrains/Toolbox/scripts',\n",
       " 'PWD': '/',\n",
       " 'SHELL': '/bin/bash',\n",
       " 'SHLVL': '3',\n",
       " 'SSH_AUTH_SOCK': '/private/tmp/com.apple.launchd.0OMYeBxWCG/Listeners',\n",
       " 'TMPDIR': '/var/folders/qh/s4x89xp56wx39j0klsspgc0c0000gn/T/',\n",
       " 'USER': 'vitsiozo',\n",
       " 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       " 'VSCODE_CODE_CACHE_PATH': '/Users/vitsiozo/Library/Application Support/Code/CachedData/5437499feb04f7a586f677b155b039bc2b3669eb',\n",
       " 'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       " 'VSCODE_CWD': '/',\n",
       " 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       " 'VSCODE_IPC_HOOK': '/Users/vitsiozo/Library/Application Support/Code/1.90-main.sock',\n",
       " 'VSCODE_NLS_CONFIG': '{\"locale\":\"en-gb\",\"osLocale\":\"en-gb\",\"availableLanguages\":{},\"_languagePackSupport\":true}',\n",
       " 'VSCODE_PID': '53664',\n",
       " 'WANDB_API_KEY': '<hidden>',\n",
       " 'XPC_FLAGS': '0x0',\n",
       " 'XPC_SERVICE_NAME': '0',\n",
       " '_': '/Users/vitsiozo/anaconda3/bin/python',\n",
       " '__CFBundleIdentifier': 'com.microsoft.VSCode',\n",
       " '__CF_USER_TEXT_ENCODING': '0x1F5:0x0:0x2',\n",
       " 'ELECTRON_RUN_AS_NODE': '1',\n",
       " 'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       " 'PYTHONUNBUFFERED': '1',\n",
       " 'PYTHONIOENCODING': 'utf-8',\n",
       " '_CE_CONDA': '',\n",
       " 'CONDA_ROOT': '/Users/vitsiozo/anaconda3',\n",
       " '_CE_M': '',\n",
       " 'LC_CTYPE': 'UTF-8',\n",
       " 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       " 'PYTHON_FROZEN_MODULES': 'on',\n",
       " 'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'FORCE_COLOR': '1',\n",
       " 'CLICOLOR_FORCE': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://matplotlib_inline.backend_inline'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dcb40b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/Users/vitsiozo/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so' could not be imported from '/Users/vitsiozo/Library/Python/3.9/lib/python/site-packages/psutil/_psutil_osx.abi3.so, 0x0002'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import langchain # Main LangChain import\n",
    "from langchain_openai import ChatOpenAI # To work with OpenAI\n",
    "from langchain_anthropic import ChatAnthropic # To work with Anthropic (optional)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI # To work with Gemini (optional)\n",
    "from langchain_core.output_parsers import JsonOutputParser # To help with structured output\n",
    "from langchain_core.prompts import PromptTemplate # To help create our prompt\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # To help with defining what output structure we want\n",
    "\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get api key for chatgpt\n",
    "OPENAI_API_KEY = os.getenv('CHATGPT_API_KEY')\n",
    "print (OPENAI_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f40ab",
   "metadata": {},
   "source": [
    "Change to the directory where the tests are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce91abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024'\n",
    "\n",
    "print (\"Files included\")\n",
    "for dirname, _, filenames in os.walk(dataset_dir):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624136a5",
   "metadata": {},
   "source": [
    "Define the evaluation and training challenges and solutions in a dictionary called task_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sets = {\n",
    "    'training' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_training_solutions.json',\n",
    "    },\n",
    "    'evaluation' : {\n",
    "        'challenges' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_challenges.json',\n",
    "        'solutions' : '/Users/vitsiozo/Desktop/MSc AI/Modules/Project/ARC/datasets/arc-prize-2024/arc-agi_evaluation_solutions.json',\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f2b78",
   "metadata": {},
   "source": [
    "Load the tasks from the files and return the challenges and solutions tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tasks_from_file(task_set):\n",
    "    \n",
    "    with open(task_set['challenges'], \"r\") as tasks:\n",
    "        challenges = json.load(tasks)\n",
    "\n",
    "    with open(task_set['solutions'], \"r\") as tasks:\n",
    "        solutions = json.load(tasks)\n",
    "\n",
    "    return challenges, solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "challenges['746b3537']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b79938",
   "metadata": {},
   "source": [
    "Setting up the LLM for access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be464ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model='gpt-4o', api_key=OPENAI_API_KEY, max_tokens=3000)\n",
    "\n",
    "## And incase you want to try Anthropic\n",
    "# llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', api_key=UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\"), max_tokens=3000)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"), max_tokens=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c577b3",
   "metadata": {},
   "source": [
    "Convert the json object into a string that can be fed to the LLM\n",
    "\n",
    "challenge_tasks: dict a list of tasks\n",
    "task_id: str the id of the task we want to convert to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_task_to_string(challenge_tasks: dict, task_id: str, test_input_index: int) -> str:\n",
    "    \n",
    "    json_task = challenge_tasks[task_id]\n",
    "\n",
    "    final_output = \"\"\n",
    "\n",
    "    train_tasks = json_task['train']\n",
    "    test_task = json_task['test']\n",
    "\n",
    "    final_output = \"Training Examples\\n\"\n",
    "\n",
    "    for i, task in enumerate(train_tasks):\n",
    "        final_output += f\"Example {i + 1}: Input\\n[\"\n",
    "        for row in task['input']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "        final_output += f\"Example {i + 1}: Output\\n[\"\n",
    "\n",
    "        for row in task['output']:\n",
    "            final_output += f\"\\n{str(row)},\"\n",
    "\n",
    "        final_output += \"]\\n\\n\"\n",
    "\n",
    "    final_output += \"Test\\n[\"\n",
    "    for row in test_task[test_input_index]['input']:\n",
    "        final_output += f\"\\n{str(row)}\"\n",
    "\n",
    "    final_output += \"]\\n\\nYour Response:\"\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf1613",
   "metadata": {},
   "source": [
    "Try this with a specific example task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a308bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_string = json_task_to_string(challenges, '746b3537', 0)\n",
    "print (task_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09debb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a prediction as a list of lists\n",
    "class ARCPrediction(BaseModel):\n",
    "    prediction: List[List] = Field(..., description=\"A prediction for a task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e24bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_prediction(challenge_tasks, task_id, test_input_index) -> List[List]:\n",
    "    \"\"\"\n",
    "    challenge_tasks: dict a list of tasks\n",
    "    task_id: str the id of the task we want to get a prediction for\n",
    "    test_input_index: the index of your test input. 96% of tests only have 1 input.\n",
    "\n",
    "    Given a task, predict the test output\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the string representation of your task\n",
    "    task_string = json_task_to_string(challenge_tasks, task_id, test_input_index)\n",
    "    \n",
    "    # Set up a parser to inject instructions into the prompt template.\n",
    "    parser = JsonOutputParser(pydantic_object=ARCPrediction)\n",
    "\n",
    "    # Create your prompt template. This is very rudimentary! You should edit this to do much better.\n",
    "    # For example, we don't tell the model what it's first attempt was (so it can do a different one), that might help!\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"You are a bot that is very good at solving puzzles. Below is a list of input and output pairs with a pattern.\" \n",
    "                    \"Identify the pattern, then apply that pattern to the test input to give a final output\"\n",
    "                    \"Just give valid json list of lists response back, nothing else. Do not explain your thoughts.\"\n",
    "                    \"{format_instructions}\\n{task_string}\\n\",\n",
    "        input_variables=[\"task_string\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Wrap up your chain with LCEL\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Optional, print out the prompt if you want to see it. If you use LangSmith you could view this there as well.\n",
    "    # print (f\"Prompt:\\n\\n{prompt.format(task_string=task_string)}\")\n",
    "    \n",
    "    # Finally, go get your prediction from your LLM. Ths will make the API call.\n",
    "    output = chain.invoke({\"task_string\": task_string})\n",
    "\n",
    "    # Because the output is structured, get the prediction key. If it isn't there, then just get the output\n",
    "    if isinstance(output, dict):\n",
    "        prediction = output.get('prediction', output)\n",
    "    else:\n",
    "        prediction = output\n",
    "\n",
    "    # Safety measure to error out if you don't get a list of lists of ints back. This will spark a retry later.\n",
    "    if not all(isinstance(sublist, list) and all(isinstance(item, int) for item in sublist) for sublist in prediction):\n",
    "        print(\"Warning: Output must be a list of lists of integers.\")\n",
    "        print (f\"Errored Output: {prediction}\")\n",
    "        raise ValueError(\"Output must be a list of lists of integers.\")\n",
    "    \n",
    "    # Let's find the shape of our prediction\n",
    "    num_rows = len(prediction)\n",
    "    num_cols = len(prediction[0]) if num_rows > 0 else 0\n",
    "    print(f\"    Prediction Grid Size: {num_rows}x{num_cols}\\n\")\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57240bf9",
   "metadata": {},
   "source": [
    "Create a function that will run through the challenges and create a submission file compatible with ARC Prize 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b7269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(challenges, NUM_ATTEMPTS=2, RETRY_ATTEMPTS=3, NUM_TASKS=None):\n",
    "    \"\"\"\n",
    "    challenges: dict a list of challenges. This should come directly from your _challenges file\n",
    "    NUM_ATTEMPTS: int the number of times to attempt a prediction. The official competition has 2 attempts.\n",
    "    RETRY_ATTEMPTS: int the number of times to retry a prediction if it fails\n",
    "    NUM_TASKS: int, If set, this represents the the number of tasks you'd like to test. If None then the all challeneges will be tested\n",
    "\n",
    "    Loop through your challenges and produce a submission.json file you can submit for a score.\n",
    "    \"\"\"\n",
    "\n",
    "    # A dict to hold your submissions that you'll return after all predictions are made\n",
    "    submission = {}\n",
    "\n",
    "    # Run through each task in your challenge set\n",
    "    for i, task_id in enumerate(challenges):\n",
    "        task_attempts = []  # List to store all attempts for the current task\n",
    "\n",
    "        # Go through each test pair to get a prediction. 96% of challenges have 1 pair.\n",
    "        for t, pair in enumerate(challenges[task_id]['test']):\n",
    "            print(f\"Starting task #{i + 1} ({task_id}), pair #{t+1}\")\n",
    "\n",
    "            # Dictionary to store attempts for the current test pair\n",
    "            pair_attempts = {}  \n",
    "\n",
    "            # Run through each prediction attempt\n",
    "            for attempt in range(1, NUM_ATTEMPTS + 1):\n",
    "                attempt_key = f\"attempt_{attempt}\"\n",
    "                pair_attempts[attempt_key] = [] # Init your attempt\n",
    "\n",
    "                # Try to get a prediction, with retries in case of failure\n",
    "                for retry in range(RETRY_ATTEMPTS):\n",
    "                    try:\n",
    "                        print(f\"    Predicting attempt #{attempt}, retry #{retry + 1}\")\n",
    "                        prediction = get_task_prediction(challenge_tasks=challenges,\n",
    "                                                         task_id=task_id,\n",
    "                                                         test_input_index=t)\n",
    "                        \n",
    "                        # If you get a valid prediction (list of lists of ints) with no error, then log the attempt\n",
    "                        pair_attempts[attempt_key] = prediction\n",
    "                        break  # Break the retry loop if prediction is successful\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retrying: {e}\")\n",
    "                        if retry == RETRY_ATTEMPTS - 1:\n",
    "                            pair_attempts[attempt_key] = []  # Assign None if all retries fail\n",
    "\n",
    "            # After you get your attempts, append them to the task attempts\n",
    "            task_attempts.append(pair_attempts)\n",
    "\n",
    "        # Append the task attempts to the submission with the task_id as the key\n",
    "        submission[task_id] = task_attempts\n",
    "\n",
    "        # If you want to stop after N tasks, uncomment the below\n",
    "        if NUM_TASKS is not None and i + 1 == NUM_TASKS:\n",
    "            break\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdca20a",
   "metadata": {},
   "source": [
    "Try this with an example from training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up training tasks\n",
    "challenges, solutions = load_tasks_from_file(task_set=task_sets['training'])\n",
    "\n",
    "# Run the model on a single task\n",
    "submission = run_model(challenges, NUM_TASKS=1)\n",
    "\n",
    "# Print the submission\n",
    "print (submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e8993f",
   "metadata": {},
   "source": [
    "Create a function that will take the submission output and save it as a submission.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(submission, file_name='submission.json'):\n",
    "    \"\"\"\n",
    "    Save a submission file to the specified file name\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\") as file:\n",
    "        json.dump(submission, file)\n",
    "\n",
    "    print (f\"Submission saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b731b4",
   "metadata": {},
   "source": [
    "Create a function to score the submission by comparing the submission.json file to the solutions file for the corresponding set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_submission(submission_file_name, solutions) -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    submission_file_name: str, the file name of your submission file\n",
    "    solutions: dict, the ground truth solutions you'd like to test against\n",
    "    \n",
    "    Read a submission from file, score it, then return the score\n",
    "    \"\"\"\n",
    "    print (f\"Scoring {submission_file_name}\\n\")\n",
    "\n",
    "    # Open your submission file\n",
    "    with open(submission_file_name, \"r\") as file:\n",
    "        submission = json.load(file)\n",
    "\n",
    "    total_score = 0\n",
    "    total_tasks = 0\n",
    "\n",
    "    # Loop through each task in your submission to grade it\n",
    "    for task_id, task_submission in submission.items():\n",
    "        total_tasks += 1\n",
    "        task_score = 0\n",
    "        num_pairs = len(task_submission)\n",
    "\n",
    "        # Go through each task. Most will only have 1\n",
    "        for pair_index, pair_attempts in enumerate(task_submission):\n",
    "            print(f\"Scoring Task {task_id} pair #{pair_index+1}\")\n",
    "            pair_correct = False\n",
    "\n",
    "            # Look at both of your attempts\n",
    "            for attempt_key, attempt in pair_attempts.items():\n",
    "                \n",
    "                # check to see if one is correct\n",
    "                if attempt == solutions[task_id][pair_index]:\n",
    "                    print(f\"Task Id {task_id} pair {pair_index+1} {attempt_key} matches solution\")\n",
    "                    pair_correct = True\n",
    "                    break # If it is correct, log it and break the loop\n",
    "\n",
    "            if pair_correct:\n",
    "                task_score += 1\n",
    "\n",
    "        task_score /= num_pairs\n",
    "        total_score += task_score\n",
    "\n",
    "    return {\n",
    "        'total_score': total_score,\n",
    "        'total_tasks_scored': total_tasks\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff2adba",
   "metadata": {},
   "source": [
    "This is a simple function that will load up the tasks, run the model, create a submission file and then score the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(task_set='training', NUM_TASKS=None, submission_file_name='submission.json'):\n",
    "    # Load datasets\n",
    "    challenges, solutions = load_tasks_from_file(task_set=task_sets[task_set])\n",
    "\n",
    "    # # Run the model\n",
    "    submission = run_model(challenges, NUM_TASKS=NUM_TASKS)\n",
    "\n",
    "    # Create (and overwrite) a submission file\n",
    "    create_submission_file(submission, file_name=submission_file_name)\n",
    "\n",
    "    # Score the submission\n",
    "    score_result = score_submission(solutions = solutions, submission_file_name=submission_file_name)\n",
    "\n",
    "    print(f\"Final score: {score_result['total_score']} of {score_result['total_tasks_scored']} ({round(score_result['total_score']/score_result['total_tasks_scored'] * 100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b592e",
   "metadata": {},
   "source": [
    "### Run the test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00893531",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(task_set='evaluation', NUM_TASKS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b2858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
